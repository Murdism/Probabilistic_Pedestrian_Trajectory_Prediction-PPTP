{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim6\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable # storing data while learning\n",
    "from config import CFG ,Args,cfg_zed\n",
    "from baselineUtils import load_datasets,distance_metrics\n",
    "from utils import ScheduledOptim,visualize_preds,visualize_preds_only\n",
    "from train import train_attn_mdn \n",
    "from test import test_mdn ,inference,inference_realtime\n",
    "from torch.optim.lr_scheduler import LambdaLR \n",
    "from model import Attention_GMM #,Attention_GMM_Encoder,Transformer_MDN\n",
    "# from torch.utils.data.distributed import  DistributedSampler\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# from torch.distributed import init_process_group,destroy_process_group\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([-6.5713e-03, -8.1075e-05])\n",
    "std = torch.tensor([0.3120, 0.1511])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class person:\n",
    "    def __init__(self,id,point_box,bbox,dist,height,frame_rate=15): # takes the id and first bounding box of the detected person\n",
    "        self.id = id\n",
    "        self.input_size = 8          # input size for point based is 8 frames\n",
    "        self.index = self.input_size-1 #self.input_size-1\n",
    "        self.distance = dist\n",
    "        self.height = height\n",
    "        self.bounding = bbox\n",
    "        self.history = np.zeros((self.input_size, 4)) # 6 parameters position(x,y)velocity(x,y),acceleration(x,y) \n",
    "        # self.history = [[0]*4]*self.input_size\n",
    "        # self.his_np = np.zeros((self.input_size, 4))\n",
    "        self.dt = 4  #1/frame_rate            # change in time\n",
    "        self.add_box(point_box)\n",
    "   \n",
    "    def derivative(self,x, dt=1):\n",
    "        not_nan_mask = ~np.isnan(x)\n",
    "        masked_x = x[not_nan_mask]\n",
    "\n",
    "        if masked_x.shape[-1] < 2:\n",
    "            return np.zeros_like(x)\n",
    "        dx = np.full_like(x, np.nan)\n",
    "    \n",
    "        dx[not_nan_mask] = np.ediff1d(masked_x, to_begin=(masked_x[1] - masked_x[0]))#/ self.dt\n",
    "        \n",
    "        return dx\n",
    "             \n",
    "\n",
    "    def add_box(self,box):\n",
    "        for j in range (self.index,self.input_size-1):\n",
    "            self.history[j] =  self.history[j+1]\n",
    "        self.history[self.input_size-1] = box\n",
    "\n",
    "        if self.index < self.input_size-1:\n",
    "            self.history[self.input_size-1][2] = self.history[self.input_size-1][0]- self.history[self.input_size-2][0]\n",
    "            self.history[self.input_size-1][3] = self.history[self.input_size-1][1]- self.history[self.input_size-2][1]\n",
    "        # self.history[self.input_size-1][4] = self.history[self.input_size-1][0]- self.history[self.input_size-2][0]\n",
    "        # self.history[self.input_size-1][5] = self.history[self.input_size-1][1]- self.history[self.input_size-2][1]\n",
    "        #print(\"HISTORY:\", self.history)\n",
    "        if self.index !=0: \n",
    "            self.index-=1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################   ZED CAMERA ###############################################################\n",
    "import pyzed.sl as sl\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from utils import generate_square_mask\n",
    "from trajectory_candidates import run_cluster\n",
    "def infer_real_time(attn_mdn,device,add_features,mean,std,normalized=True,enc_seq = 8,dec_seq=12):\n",
    "    # Create a Camera object\n",
    "    zed = sl.Camera()\n",
    "    zed_pose = sl.Pose()\n",
    "\n",
    "    # Create a InitParameters object and set configuration parameters\n",
    "    init_params = sl.InitParameters()\n",
    "    init_params.camera_resolution = sl.RESOLUTION.HD720  # Use HD1080 video mode    \n",
    "    init_params.coordinate_units = sl.UNIT.METER\n",
    "    init_params.camera_fps = cfg_zed.frame_rate #cfg_zed['frame_rate']                          # Set fps at 30\n",
    "    init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP\n",
    "    #camera_position = zed.get_position(zed_pose, sl.REFERENCE_FRAME.CAMERA)\n",
    "\n",
    "    # Set runtime parameters\n",
    "    runtime_parameters = sl.RuntimeParameters()\n",
    "\n",
    "    # Open the camera\n",
    "    err = zed.open(init_params)\n",
    "    if err != sl.ERROR_CODE.SUCCESS:\n",
    "        exit(1)\n",
    "\n",
    "    # Enable object detection module\n",
    "    obj_param = sl.ObjectDetectionParameters()\n",
    "    # Defines if the object detection will track objects across images flow.\n",
    "    obj_param.enable_tracking = True       # if True, enable positional tracking\n",
    "\n",
    "    if obj_param.enable_tracking:\n",
    "        zed.enable_positional_tracking()\n",
    "        \n",
    "    zed.enable_object_detection(obj_param)\n",
    "\n",
    "    camera_info = zed.get_camera_information()\n",
    "    # Create OpenGL viewer\n",
    "    # viewer = gl.GLViewer()\n",
    "    # viewer.init(camera_info.calibration_parameters.left_cam, obj_param.enable_tracking)\n",
    "\n",
    "    # Configure object detection runtime parameters\n",
    "    obj_runtime_param = sl.ObjectDetectionRuntimeParameters()\n",
    "    obj_runtime_param.detection_confidence_threshold = 60\n",
    "    obj_runtime_param.object_class_filter = [sl.OBJECT_CLASS.PERSON]    # Only detect Persons\n",
    "\n",
    "    # Create ZED objects filled in the main loop\n",
    "    objects = sl.Objects()\n",
    "    image = sl.Mat()\n",
    "    objects_dic= dict()          # Dictionary holding objects values        \n",
    "\n",
    "    while zed.grab(runtime_parameters) == sl.ERROR_CODE.SUCCESS:#viewer.is_available():\n",
    "        # Grab an image, a RuntimeParameters object must be given to grab()\n",
    "        # if zed.grab(runtime_parameters) == sl.ERROR_CODE.SUCCESS:\n",
    "        # Retrieve left image\n",
    "        zed.retrieve_image(image, sl.VIEW.LEFT)\n",
    "        image_ocv = image.get_data() \n",
    "        # Retrieve objects\n",
    "        zed.retrieve_objects(objects, obj_runtime_param)\n",
    "        batch_dic=dict()    \n",
    "        input_list =[] \n",
    "        bbox_list =[]\n",
    "        dists = []\n",
    "        object_added = False      \n",
    "        if objects.is_new:\n",
    "            # Count the number of objects detected\n",
    "            print(\"{} Object(s) detected\".format(len(objects.object_list)))\n",
    "            if len(objects.object_list):\n",
    "                obj_detected= False\n",
    "                \n",
    "                for obj in objects.object_list:\n",
    "                    obj_detected= True\n",
    "                    distance = math.sqrt(obj.position[0]*obj.position[0] + obj.position[2]*obj.position[2])\n",
    "                    box = [obj.position[0],obj.position[2],0,0]\n",
    "                    height = obj.position[1]\n",
    "                    bounding_box = obj.bounding_box_2d \n",
    "                    frame_rate = zed.get_camera_information().camera_fps\n",
    "                    bbox = np.concatenate((bounding_box[0], bounding_box[2]), axis=0)\n",
    "                    #print(\"bboxbboxbboxbboxbbox: \",bbox)\n",
    "                    start_point = (int(bbox[0]),int(bbox[1]))\n",
    "                    end_point = (int(bbox[2]),int(bbox[3]))\n",
    "                    #cv2.rectangle(image_ocv, start_point, end_point,cfg_zed.crossing_color, cfg_zed.line_thickness)\n",
    "\n",
    "\n",
    "                    key = obj.id\n",
    "                    velocity_is_nan = math.isnan(obj.velocity[0])\n",
    "\n",
    "                    if key in objects_dic: \n",
    "                        object_added = True\n",
    "                        objects_dic[key].add_box(box)\n",
    "                        objects_dic[key].bbox = bbox\n",
    "                        batch_dic[key] =  objects_dic[key]\n",
    "                        input_list.append(objects_dic[key].history)\n",
    "                        bbox_list.append(bbox)\n",
    "                        dists.append(distance)\n",
    "                    else:\n",
    "                        new_person = person(key,box,bbox,distance,height,frame_rate)\n",
    "                        batch_dic[key] = new_person\n",
    "                    # elif (not velocity_is_nan):\n",
    "                    #     new_person = person(key,box,bbox,distance,height,frame_rate)\n",
    "                    #     batch_dic[key] = new_person\n",
    "                \n",
    "                objects_dic = batch_dic\n",
    "        input_dataset = torch.Tensor(np.array(input_list))\n",
    "        if object_added:\n",
    "            print(\"shape:\",len(input_dataset),len(input_dataset[0]),len(input_dataset[0][0]))\n",
    "            print(\"bbox_list:\",len(bbox_list),len(bbox_list[0]))\n",
    "        #test_dl = torch.utils.data.DataLoader(input_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
    "            with torch.no_grad():\n",
    "                attn_mdn.eval()\n",
    "                \n",
    "                #input_valc = torch.sqrt(torch.square(input_dataset[:,1:,2].to(device)) + torch.square(input_dataset[:,1:,3].to(device))).unsqueeze(-1)\n",
    "            # for id_e, val_batch in enumerate(test_dl):\n",
    "            #     pass\n",
    "                # inp_val=(val_batch[:,1:,2:4].to(device)-mean.to(device))/std.to(device)\n",
    "                # input_valc = torch.sqrt(torch.square(val_batch[:,1:,2].to(device)) + torch.square(val_batch[:,1:,3].to(device))).unsqueeze(-1)\n",
    "                if normalized:\n",
    "                    inp_val=(input_dataset[:,1:,2:4].to(device)-mean.to(device))/std.to(device)\n",
    "                    \n",
    "                \n",
    "                else :\n",
    "                    inp_val = input_dataset[:,1:,2:4].to(device)\n",
    "\n",
    "                if (add_features):                \n",
    "                    input_valc = torch.sqrt(torch.square(input_dataset[:,1:,2].to(device)) + torch.square(input_dataset[:,1:,3].to(device))).unsqueeze(-1)\n",
    "                    input_val = torch.cat((inp_val,input_valc),-1)\n",
    "                else:\n",
    "                    input_val = inp_val\n",
    "                tgt_val = torch.Tensor([0, 0]).unsqueeze(0).unsqueeze(1).repeat(input_val.shape[0],12,1).to(device)\n",
    "                tgt_val_mask = generate_square_mask(dim_trg = dec_seq ,dim_src = enc_seq, mask_type=\"tgt\").to(device)\n",
    "                pi, sigma_x,sigma_y, mu_x , mu_y,decoder_out = attn_mdn(input_val,tgt_val,tgt_mask = tgt_val_mask)\n",
    "                mus = torch.cat((mu_x.unsqueeze(-1),mu_y.unsqueeze(-1)),-1)\n",
    "                #sigmas = torch.cat((sigma_x.unsqueeze(-1),sigma_y.unsqueeze(-1)),-1)\n",
    "\n",
    "                src_value = input_dataset[:, -1:,0:2].detach().cpu().numpy()\n",
    "                src_value = src_value[:,np.newaxis,:,:]\n",
    "                cluster_mus = (mus[:, :,:] * std.to(device) + mean.to(device)).detach().cpu().numpy().cumsum(1) + src_value\n",
    "\n",
    "                # cluster_real = val_batch['trg'][:, :, 0:2]\n",
    "                cluster_real = input_dataset[:, :, 0:2]\n",
    "                cluster_src = input_dataset[:,:,0:2]\n",
    "                batch_trajs,batch_weights,best_trajs,best_weights = run_cluster(cluster_mus,pi,cluster_real,cluster_src,dbscan=False)\n",
    "                print(\"best_candiates: \",best_trajs,best_weights)\n",
    "            \n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontScale = 1\n",
    "            for trajs,traj,bbox,wgt,dist in zip (batch_trajs,best_trajs,bbox_list,best_weights,dists):\n",
    "                start_point = (int(bbox[0]),int(bbox[1]))\n",
    "                end_point = (int(bbox[2]),int(bbox[3]))\n",
    "                # half_w = int(int(bbox[0])+int(bbox[2])/2)\n",
    "                # half_h = int(int(bbox[1])+int(bbox[3])/2)\n",
    "                # org = (half_w, half_h)\n",
    "                count = sum(n[0] < 0 for n in traj)\n",
    "                # print(\"count: \",count)\n",
    "                # print(\"traj: \",traj)\n",
    "                if wgt>0.99:\n",
    "                    wgt = 0.98\n",
    "                kind = str(int(wgt*100)) + \"% Safe!\"\n",
    "                if (dist < cfg_zed.safe_distance):\n",
    "                    print(\"Dangerously Close to Vehicle!!\")\n",
    "                    cv2.rectangle(image_ocv, start_point, end_point,cfg_zed.danger_color, cfg_zed.line_thickness)\n",
    "                    kind = str(int(wgt*100)) + \"% DANGER!\"\n",
    "                elif (count!=0 and count!=12)and (dist < cfg_zed.safe_distance_lvl2) :\n",
    "                     print(\"Crossing Red!\",dist)\n",
    "                     cv2.rectangle(image_ocv, start_point, end_point,cfg_zed.crossing_color, cfg_zed.line_thickness)\n",
    "                     kind = str(int(wgt*100)) + \"% Crossing Red!\"\n",
    "                elif (count!=0 and count!=12):# and (dist < cfg_zed.max_cross_detection)) :\n",
    "                     print(\"Crossing Blue!\",dist)\n",
    "                     cv2.rectangle(image_ocv, start_point, end_point,cfg_zed.crossing_color, cfg_zed.line_thickness)\n",
    "                     kind = str(int(wgt*100)) + \"% Crossing Blue!!\"\n",
    "                else:\n",
    "                     print(\"SAFE\")\n",
    "                     cv2.rectangle(image_ocv, start_point, end_point,cfg_zed.safe_color, cfg_zed.line_thickness) \n",
    "\n",
    "                #cv2.rectangle(image_ocv, start_point, end_point,cfg_zed.crossing_color, cfg_zed.line_thickness)\n",
    "                # Using cv2.putText() method\n",
    "                cv2.putText(image_ocv, kind, start_point, font, \n",
    "                                fontScale, cfg_zed.crossing_color, cfg_zed.line_thickness, cv2.LINE_AA)\n",
    "                \n",
    "            \n",
    "                \n",
    "        #batch_preds,candidate_trajs,candidate_weights,best_candiates,best_candiates_weights,src_trajs = inference_realtime(test_dl, attn_mdn,device,add_features = add_features,mixtures=gaussians,enc_seq = 8,dec_seq=12, mode='feed',loss_mode ='mdn',mean=mean,std=std)\n",
    "        \n",
    "        #visualize_preds_only(src_trajs,candidate_trajs,candidate_weights)\n",
    "        #print(\"INPUT LIST: \",input_list)\n",
    "            # for dic_key in objects_dic:\n",
    "            #     if (objects_dic[dic_key].index < 5):             # wait at least until 4 frames of a person are detected..index decreases starting  from 8\n",
    "        # Update GL view\n",
    "        #image_ocv = image.get_data() \n",
    "        cv2.imshow(\"Image\", image_ocv)\n",
    "        cv2.waitKey(1)\n",
    "    image.free(memory_type=sl.MEM.CPU)\n",
    "    zed.disable_object_detection()\n",
    "    zed.disable_positional_tracking()\n",
    "    cv2.destroyAllWindows()\n",
    "    zed.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device used and Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = CFG.device\n",
    "batch_size = CFG.batch_size\n",
    "print(f\"Using {device} device\")\n",
    "args = Args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Args.real_time:\n",
    "    train_dataset, val_dataset,test_dataset,mean,std = load_datasets(args)\n",
    "    train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
    "    val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
    "    test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "loss_type = Args.loss_mode\n",
    "in_features = CFG.in_features\n",
    "out_features = CFG.out_features\n",
    "num_heads = CFG.num_heads\n",
    "num_encoder_layers = CFG.num_encoder_layers\n",
    "num_decoder_layers =  CFG.num_decoder_layers\n",
    "embedding_size = CFG.embd_size\n",
    "max_length = 8\n",
    "n_hidden = CFG.n_hidden\n",
    "gaussians = CFG.gaussians\n",
    "forecast_window = 12\n",
    "drp = CFG.drop_out\n",
    "add_features = CFG.add_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to train transformer only copy it from commented.py\n",
    "# Train attention MDN\n",
    "#attn_mdn = Attention_GMM(device,in_features,out_features,num_heads,num_encoder_layers,num_decoder_layers,embedding_size,n_gaussians=gaussians,n_hidden = n_hidden, dropout=drp).to(device)\n",
    "attn_mdn = Attention_GMM(in_features,out_features,num_heads,num_encoder_layers,num_decoder_layers,embedding_size,n_gaussians=gaussians,n_hidden = n_hidden, dropout=drp).to(device)\n",
    "#attn_mdn = Attention_GMM_Encoder(device,in_features,out_features,num_heads,num_encoder_layers,num_decoder_layers,embedding_size,n_gaussians=gaussians,n_hidden = n_hidden, dropout=drp).to(device)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "#     attn_mdn = DDP(attn_mdn,device_ids=[0,1])\n",
    "for p in attn_mdn.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "# Define the optimizer\n",
    "optimizer = ScheduledOptim(\n",
    "        torch.optim.Adam(attn_mdn.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "        CFG.lr_mul, CFG.d_model, CFG.n_warmup_steps) #len(train_dl)True\n",
    "#         print(name, child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Args.mode=='train'):\n",
    "    loss_train, loss_eval,val_mad,val_fad = train_attn_mdn(train_dl,val_dl,test_dl,attn_mdn,optimizer,add_features,mixtures =gaussians, epochs=CFG.epochs,mean=mean,std=std)\n",
    "else:\n",
    "    PATH = Args.model_path\n",
    "    print(\"PATH: \",PATH)\n",
    "    attn_mdn = torch.load(PATH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Args.mode=='train'):\n",
    "    fig = plt.figure(1)\t#identifies the figure \n",
    "    plt.title(\" Training Loss Per Epoch\", fontsize='16')\t#title\n",
    "    plt.plot(loss_train,color='Blue',label='Training Loss')\t#plot the points\n",
    "    plt.plot(loss_eval,color='Green',label='Evaluation Loss')\t#plot the points\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Args.mode=='train'):\n",
    "    fig = plt.figure(2)\t#identifies the figure \n",
    "    plt.title(\"Evaluation Error\", fontsize='16')\t#title\n",
    "    # plt.plot(test_mad,color='Green', label=\"ADE Test\")\t#plot the points\n",
    "    # plt.plot(test_fad,color='Red', label=\"FDE Test\")\t#plot the points\n",
    "    plt.plot(val_mad,color='Green', label=\"ADE Validation\")\t#plot the points\n",
    "    plt.plot(val_fad,color='Red', label=\"FDE Validation\")\t#plot the points\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in attn_mdn.parameters() if p.requires_grad)\n",
    "print(f\"The model has {num_params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Args.mode=='inference'):\n",
    "    if Args.real_time:\n",
    "        print(\"REAL_TIME PREDICTION\")\n",
    "        infer_real_time(attn_mdn,device,add_features,mean,std)\n",
    "        \n",
    "    else:\n",
    "        batch_preds,candidate_trajs,candidate_weights,best_candiates,src_trajs = inference(test_dl, attn_mdn,device,add_features = add_features,mixtures=gaussians,enc_seq = 8,dec_seq=12, mode='feed',loss_mode ='mdn',mean=mean,std=std)\n",
    "else:\n",
    "    # Test the model\n",
    "    batch_preds,batch_gts,avg_mad,avg_fad,candidate_trajs,candidate_weights,best_candiates,src_trajs = test_mdn(test_dl, attn_mdn,device,add_features = add_features,mixtures=gaussians,enc_seq = 8,dec_seq=12, mode='feed',loss_mode ='mdn',mean=mean,std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the model\n",
    "# batch_preds,batch_gts,avg_mad,avg_fad,candidate_trajs,candidate_weights,best_candiates,src_trajs = test_mdn(test_dl, attn_mdn,device,add_features = add_features,mixtures=gaussians,enc_seq = 8,dec_seq=12, mode='feed',loss_mode ='mdn',mean=mean,std=std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape of candidate trajs (num_batchs, bacth_size, x, 2, 12, 2)  ---> where x is the number of candidate trajectories\n",
    "shape of ground truth (num_batchs, bacth_size, 12, 2) ---> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulaize output via visualize_preds\n",
    "if Args.visualize and Args.mode=='inference':\n",
    "    visualize_preds_only(src_trajs,candidate_trajs,candidate_weights)\n",
    "elif Args.visualize:\n",
    "    visualize_preds(src_trajs,batch_gts,candidate_trajs,candidate_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visulaize output via visualize_preds\n",
    "# if Args.visualize:\n",
    "#     visualize_preds(src_trajs,batch_gts,candidate_trajs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97275231ca3bc88d4d425f48fe4700083adb1bbc86d99b7fab9639fa13d0f98b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
