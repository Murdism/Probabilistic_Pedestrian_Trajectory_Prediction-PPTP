{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avl1/anaconda3/envs/mdn/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim6\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable # storing data while learning\n",
    "from config import CFG ,Args\n",
    "from baselineUtils import load_datasets,distance_metrics\n",
    "from utils import ScheduledOptim,visualize_preds\n",
    "from train import train_attn_mdn \n",
    "from test import test_mdn \n",
    "from torch.optim.lr_scheduler import LambdaLR \n",
    "from model import Attention_GMM #,Attention_GMM_Encoder,Transformer_MDN\n",
    "# from torch.utils.data.distributed import  DistributedSampler\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# from torch.distributed import init_process_group,destroy_process_group\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device used and Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1 device\n"
     ]
    }
   ],
   "source": [
    "device = CFG.device\n",
    "batch_size = CFG.batch_size\n",
    "print(f\"Using {device} device\")\n",
    "args = Args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading dataset\n",
      "validation set size -> 0\n",
      "001 / 007 - loading crowds_zara01_train.txt\n",
      "002 / 007 - loading crowds_zara03_train.txt\n",
      "003 / 007 - loading students003_train.txt\n",
      "004 / 007 - loading students001_train.txt\n",
      "005 / 007 - loading uni_examples_train.txt\n",
      "006 / 007 - loading biwi_eth_train.txt\n",
      "007 / 007 - loading biwi_hotel_train.txt\n",
      "start loading dataset\n",
      "validation set size -> 0\n",
      "001 / 007 - loading biwi_eth_val.txt\n",
      "002 / 007 - loading students001_val.txt\n",
      "003 / 007 - loading uni_examples_val.txt\n",
      "004 / 007 - loading crowds_zara03_val.txt\n",
      "005 / 007 - loading crowds_zara01_val.txt\n",
      "006 / 007 - loading biwi_hotel_val.txt\n",
      "007 / 007 - loading students003_val.txt\n",
      "start loading dataset\n",
      "validation set size -> 0\n",
      "001 / 001 - loading crowds_zara02.txt\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset,test_dataset,mean,std = load_datasets(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
    "val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "in_features = CFG.in_features\n",
    "out_features = CFG.out_features\n",
    "num_heads = CFG.num_heads\n",
    "num_encoder_layers = CFG.num_encoder_layers\n",
    "num_decoder_layers =  CFG.num_decoder_layers\n",
    "embedding_size = CFG.embd_size\n",
    "max_length = 8\n",
    "n_hidden = CFG.n_hidden\n",
    "gaussians = CFG.gaussians\n",
    "forecast_window = 12\n",
    "drp = CFG.drop_out\n",
    "add_features = CFG.add_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to train transformer only copy it from commented.py\n",
    "# Train attention MDN\n",
    "attn_mdn = Attention_GMM(device,in_features,out_features,num_heads,num_encoder_layers,num_decoder_layers,embedding_size,n_gaussians=gaussians,n_hidden = n_hidden, dropout=drp).to(device)\n",
    "#attn_mdn = Attention_GMM_Encoder(device,in_features,out_features,num_heads,num_encoder_layers,num_decoder_layers,embedding_size,n_gaussians=gaussians,n_hidden = n_hidden, dropout=drp).to(device)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "#     attn_mdn = DDP(attn_mdn,device_ids=[0,1])\n",
    "for p in attn_mdn.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "# Define the optimizer\n",
    "optimizer = ScheduledOptim(\n",
    "        torch.optim.Adam(attn_mdn.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "        CFG.lr_mul, CFG.d_model, CFG.n_warmup_steps) #len(train_dl)True\n",
    "#         print(name, child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Args.mode=='test'):\n",
    "    PATH = Args.model_path\n",
    "    attn_mdn = torch.load(PATH).to(device)\n",
    "else:\n",
    "    loss_train, loss_eval,val_mad,val_fad = train_attn_mdn(train_dl,val_dl,test_dl,attn_mdn,optimizer,add_features,mixtures =gaussians, epochs=CFG.epochs,mean=mean,std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Args.mode=='train'):\n",
    "    fig = plt.figure(1)\t#identifies the figure \n",
    "    plt.title(\" Training Loss Per Epoch\", fontsize='16')\t#title\n",
    "    plt.plot(loss_train,color='Blue',label='Training Loss')\t#plot the points\n",
    "    plt.plot(loss_eval,color='Green',label='Evaluation Loss')\t#plot the points\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Args.mode=='train'):\n",
    "    fig = plt.figure(2)\t#identifies the figure \n",
    "    plt.title(\"Evaluation Error\", fontsize='16')\t#title\n",
    "    # plt.plot(test_mad,color='Green', label=\"ADE Test\")\t#plot the points\n",
    "    # plt.plot(test_fad,color='Red', label=\"FDE Test\")\t#plot the points\n",
    "    plt.plot(val_mad,color='Green', label=\"ADE Validation\")\t#plot the points\n",
    "    plt.plot(val_fad,color='Red', label=\"FDE Validation\")\t#plot the points\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 60 / 60: 100%|██████████| 60/60 [00:06<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: -2.225660828019803\n",
      "Test loss with msq: -1.0202178960045178\n",
      "Test avg_mad: 0.3191342531281795\n",
      "Test avg_fad: 0.7018245713246919\n",
      "Test avg_mad_best: 0.3198853881146412\n",
      "Test avg_fad_best: 0.7181163657346215\n",
      "Test avg_acc_mad: [0.32833922]\n",
      "Test avg_acc_fad: [0.72632706]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "batch_preds,batch_gts,avg_mad,avg_fad,candidate_trajs,candidate_weights,best_candiates,src_trajs = test_mdn(test_dl, attn_mdn,device,add_features = add_features,mixtures=gaussians,enc_seq = 8,dec_seq=12, mode='feed',loss_mode ='mdn',mean=mean,std=std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape of candidate trajs (num_batchs, bacth_size, x, 2, 12, 2)  ---> where x is the number of candidate trajectories\n",
    "shape of ground truth (num_batchs, bacth_size, 12, 2) ---> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulaize output via visualize_preds\n",
    "if Args.visualize:\n",
    "    visualize_preds(src_trajs,batch_gts,candidate_trajs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97275231ca3bc88d4d425f48fe4700083adb1bbc86d99b7fab9639fa13d0f98b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
